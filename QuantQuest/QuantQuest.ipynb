{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This presentation aims at prepairing group members with some tools and knowledge to take the upcoming Quant Quest challenge.\n",
    "\n",
    "## Some Tools\n",
    "This section details some Python libraries that might be helpful\n",
    "1. Numerical analysis\n",
    "  * [numpy](http://www.numpy.org/) - Linear algebra, matrix and vector manipulation\n",
    "  * [pandas](http://pandas.pydata.org/) - Data anaysis, data manipulation\n",
    "2. Machine learning\n",
    "  * [scikit-learn](http://scikit-learn.org/stable/) - General machine learning. Supports basic/advance level algorithms, but only run on CPU.\n",
    "  * [theano](http://deeplearning.net/software/theano/) - Deep learning framework.\n",
    "  * [tensorflow](https://www.tensorflow.org/) - Another deep learning framework.\n",
    "3. Natural language processing\n",
    "  * [nltk](http://www.nltk.org/) - General NLP\n",
    "  * [gensim](https://radimrehurek.com/gensim/) - Topic modeling\n",
    "4. Utilities\n",
    "  * [beautiful soup](https://www.crummy.com/software/BeautifulSoup/) - Utility for working with text\n",
    "  * [urllib](https://docs.python.org/2/library/urllib2.html) - Dealing with url, lightweight scraping.\n",
    "  * [wikipedia](https://wikipedia.readthedocs.io/en/latest/quickstart.html) - Scraping from wikipedia\n",
    "  \n",
    "## Download\n",
    "You can get most of these libraries from the [Anaconda distribution](https://www.continuum.io/downloads) or from the links above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline\n",
    "1. Obtain data\n",
    "  * Either from scraping, downloading, or other means.\n",
    "2. Preprocess data\n",
    "  * Remove unwanted data.\n",
    "  * Filter out noise.\n",
    "  * Patitioning data into *training set*, *validation set*, *test set*\n",
    "  * Scale, shift, and normalize.\n",
    "3. Find a good representation\n",
    "  * The purpose of this step is to find a more representative representation of the data. \n",
    "  * In NLP, a good representation can be *word count*, or *tf-idf*.\n",
    "  * Dimensionality reduction.\n",
    "4. Training the classifier/regressor\n",
    "  * People often [k-fold cross-validation](https://www.cs.cmu.edu/~schneide/tut5/node42.html).\n",
    "  * *training* is done using gradient descent.\n",
    "  * Hyper-parameters tuning.\n",
    "5. Testing\n",
    "  * Accuracy, false-positive, false-negative, f-1 score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain data\n",
    "This section will introduce basic tools to download text corpus from wikipedia articles. We will download the content of all 500 articles of 500 companies in the S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import wikipedia as wk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def initOpener():\n",
    "    opener = urllib2.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    return opener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function bellow output a dictionary whose keys are *stock tickers* and values are article *titles*. These *titles* are then used for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSP500Dictionary():\n",
    "    stockTickerUrl = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "    usableStockTickerURL = initOpener().open(stockTickerUrl).read()\n",
    "\n",
    "    stockTickerSoup = BeautifulSoup(usableStockTickerURL, 'html.parser')\n",
    "\n",
    "    stockTickerTable = stockTickerSoup.find('table')\n",
    "\n",
    "    stockTickerRows = stockTickerTable.find_all('tr')\n",
    "\n",
    "    SP500companies = {}\n",
    "\n",
    "    stockBaseURL = 'https://en.wikipedia.org'\n",
    "\n",
    "    for stockTickerRow in stockTickerRows:\n",
    "        stockTickerColumns = stockTickerRow.find_all('td')\n",
    "        counter = 1\n",
    "        for element in stockTickerColumns:\n",
    "            # Stock Ticker\n",
    "            if (counter % 8) == 1:\n",
    "                stockTicker = element.get_text().strip().encode('utf-8', 'ignore')\n",
    "                counter = counter + 1\n",
    "            # Corresponding link to wiki page\n",
    "            elif (counter % 8 == 2):\n",
    "                SP500companies[stockTicker] = element.find('a', {'href': True}).get('href')\n",
    "                counter = counter + 1\n",
    "\n",
    "    return SP500companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell bellow uses *wikipedia* package to load the summary paragraph of the wikipedia article of each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import wikipedia as wk\n",
    "import sys\n",
    "import json\n",
    "\n",
    "SP500dict = getSP500Dictionary()\n",
    "err = []\n",
    "data = []\n",
    "comp_name = []\n",
    "for k, v in SP500dict.iteritems():\n",
    "    # k: ticker, v: company name\n",
    "    v_str = str(v)\n",
    "    pageId = v_str.split('/')[-1]\n",
    "    pageId = pageId.replace('_',' ')\n",
    "    try:\n",
    "        data.append(wk.summary(pageId).encode('utf-8'))\n",
    "        comp_name.append(pageId.encode('utf-8'))\n",
    "    except:\n",
    "        err.append((k,v))\n",
    "# Dump the data into json file for later use\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump((data, comp_name), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BorgWarner Inc. is an American worldwide automotive industry components and parts supplier. It is primarily known for its powertrain products, which include manual and automatic transmissions and transmission components, such as electro-hydraulic control components, transmission control units, friction materials, and one-way clutches, turbochargers, engine valve timing system components, along with four-wheel drive system components.\n",
      "The company has 60 manufacturing facilities across 18 countries, including the U.S., Canada, Europe, and Asia. It provides drivetrain components to all three U.S. automakers, as well as a variety of European and Asian original equipment manufacturer (OEM) customers. BorgWarner has diversified into several automotive-related markets (1999), including ignition interlock technology (ACS Corporation est.1976) for preventing impaired operation of vehicles.\n",
      "Historically, BorgWarner was also known for its ownership of the Norge appliance company (washers and dryers).\n",
      "-----\n",
      "United Continental Holdings, Inc. (formerly UAL Corporation) is a publicly traded airline holding company headquartered in the Willis Tower in Chicago. UCH owns and operates United Airlines, Inc. The company is the successor of UAL Corporation, which agreed to change its name to United Continental Holdings in May 2010, when a merger agreement was reached between United and Continental Airlines. Its stock trades under the UAL symbol. To effect the merger, Continental shareholders received 1.05 shares of UAL stock for each Continental share, effectively meaning Continental was acquired by UAL Corporation; at the time of closing, it was estimated that United shareholders owned 55% of the merged entity and Continental shareholders owned 45%. The company or its subsidiary airlines also have several other subsidiaries. Once completely combined, United became the world's largest airline, as measured by revenue passenger miles. United is a founding member of the Star Alliance.\n",
      "UCH has major operations at Chicago–O'Hare, Denver, Guam, Houston–Intercontinental, Los Angeles, Newark (New Jersey), San Francisco, Tokyo–Narita and Washington–Dulles. UCH's United Air Lines, Inc. controls several key air rights, including being one of only two American carriers authorized to serve Asia from Tokyo-Narita (the other being Delta Air Lines). Additionally, UCH's United is the largest U.S. carrier to the People’s Republic of China and maintains a large operation throughout Asia.\n",
      "UCH uses Continental's operating certificate and United's repair station certificate, having been approved by the FAA on November 30, 2011.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json') as json_data:\n",
    "    data_ = json.load(json_data)\n",
    "data = data_[0]\n",
    "comp_name = data_[1]\n",
    "# print 2 companies\n",
    "print data[10]\n",
    "print '-----'\n",
    "print data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Representation\n",
    "Vectorize documents to matrix of occurence. While counting, filter out stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498, 7940)\n"
     ]
    }
   ],
   "source": [
    "# Import the method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initialize the vectorizer with the option of stopword, which will eliminate \n",
    "# common words like 'the', 'a', etc.\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "# fit_transform method applies the vectorizer on the data set\n",
    "X_train_counts = count_vect.fit_transform(data)\n",
    "# The resulting matrix is 496 by 7942. Each row is a document (a wikipedia article)\n",
    "# each column is the occurence of each word.\n",
    "print X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf(t,d)$ is the frequency that term $t$ appears in document $d$.\n",
    "\n",
    "$df(d,t)$ is the number of documents that contain term $t$.\n",
    "\n",
    "$idf(t)=\\log \\frac{1+n_d}{1+df(d,t)} + 1$, \n",
    "  * $n_d$ is number of documents\n",
    "\n",
    "$tfidf(t,d)=tf(t,d)\\times idf(t)$\n",
    "\n",
    "In sklearn implementation, the final tfidf vector is normalized by the L2 norm.\n",
    "\n",
    "Tfidf gives a nice numerical representation of the document. From this representation, we can perform numerical analysis technique on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498, 7940)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer()\n",
    "X_train_tf = tf_transformer.fit_transform(X_train_counts)\n",
    "print X_train_tf.shape\n",
    "print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "K-means cluster your dataset into K centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=90, n_init=10,\n",
       "    n_jobs=-1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Note that n_clusters is number of cluster. This is important for accuracy. Play around with it\n",
    "classifier = KMeans(n_clusters = 90, n_jobs=-1)\n",
    "classifier.fit(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42 30 60 40 41 39 14 51 52 68 29 53 10  3 79 42 49 14 72 50 58 26 68 65 26\n",
      " 49 71  4 68 69 76 37 70 40 79 50 41 48 26 28 23 70 82 76 73  8 56 37 85 40\n",
      " 80 37  4 11 52 54 48 32 48  0 28 83 11 73 30 15 11 42 62 61 59 31 73 30  7\n",
      " 19 83 42 43 48 35 52 62 14 76 26 75 28 36 79 39 52  4 14 36 49 15 41  2 15\n",
      " 83 60 39 12 53 65 17 35 16 88 55 26 16  3 17 49 60  8  8 18 89 75 12 12 42\n",
      " 12  2 40  8  8 40 17 24 44 89 24  0  8 23 35 73  8 68 34 89 80 12 62  3  8\n",
      " 87 89 20 81 58 65 18 81  6 86 33 73 26 32 65 14 81 81 53 30 19 18  8 22 31\n",
      " 78 20 79 59 61 75 12 46 52 81 77 67 70 38  9 79 51 12 36 71 35  3 54  1 89\n",
      " 11 26  5 88 47  2 87 75  7 80 51 25 79 32 88 28 26 46 67  6 79 40 28 43 33\n",
      " 66 85 58 14 10  3  8 16 55 69  7 68 30 14 22  4  9 67 19  8 70 15 38 43 63\n",
      " 60 65 26 17 68  2 73 19 27  8 82 88  8 11 15 31 67 16 40 44 13 79 79 12 14\n",
      " 41 24 44  9 39 71 72  7  8 68 81  9 51 70 83 52 69 18  8 63 60 81 79 18 35\n",
      " 85 40 76 49 60 55 35 12  4 79 22 24 37 64 23 26 62 33  8  3 37 74 83 16 74\n",
      "  5 13 30 76 59 85 81 30 35 17  7 57 28 30  8 20 67 23 84 36 19 14 32 53 40\n",
      " 81 12 16 57 75 22 51 26 29 14 79 31 10 40 24 12 67 68 60 45 32  3 31 77 88\n",
      " 25 56 30  0 34 58 21 22 64  3  8 29 50 21 67 22 44 88 54  8 82 45  6 20 23\n",
      "  2  6 73 18 40 25  8 18 89 14  6  6 30 52 71 12 28 30 24 24 42 39  9 58 84\n",
      " 78 66 37 22 35  2 28 42 60 32 39  3 69  6 55 79 34 15  4 12 81 18 10 79 24\n",
      " 88 23 35 83 30 57 81 81 59 71 27 27 17  7 40 18 59 31  5 63 72  3  8  9 44\n",
      " 31 49 59 53 46 26 65 30 39 79  2 88  4 73  9 32 40  3 60 42 64  3 22]\n"
     ]
    }
   ],
   "source": [
    "print (classifier.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'O%27Reilly Automotive', u'Advance Auto Parts', u'Delphi Automotive', u'Genuine Parts', u'LKQ Corporation', u'AutoZone Inc', u'AutoNation Inc']\n",
      "____\n",
      "[u'Under Armour', u'Xilinx Inc', u'McCormick %26 Co.', u'Under Armour']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "group1 = np.where(classifier.labels_==9)[0]\n",
    "print [comp_name[x] for x in group1]\n",
    "print \"____\"\n",
    "print [comp_name[x] for x in np.where(classifier.labels_==10)[0]]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
